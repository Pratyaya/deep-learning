{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and string cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ubuntu/hw2-sentiment-classification-groverpr/aclImdb/train'),\n",
       " PosixPath('/home/ubuntu/hw2-sentiment-classification-groverpr/aclImdb/README'),\n",
       " PosixPath('/home/ubuntu/hw2-sentiment-classification-groverpr/aclImdb/test'),\n",
       " PosixPath('/home/ubuntu/hw2-sentiment-classification-groverpr/aclImdb/imdbEr.txt'),\n",
       " PosixPath('/home/ubuntu/hw2-sentiment-classification-groverpr/aclImdb/imdb.vocab')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/home/ubuntu/hw2-sentiment-classification-groverpr/aclImdb\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    \"\"\"\n",
    "    Read text from all files present in path and save them in list of lists\n",
    "    \"\"\"\n",
    "    \n",
    "    all_content = []\n",
    "    for file in os.listdir(path):\n",
    "        with open(path/file, encoding = \"ISO-8859-1\") as f:\n",
    "            content = list(f.readlines())\n",
    "            all_content.append(content)\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):\n",
    "    \"\"\"Tokenization/string cleaning. \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = clean_str(line.strip())\n",
    "            words = set(line.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev = read_files(PATH/\"train/pos/\")\n",
    "neg_rev = read_files(PATH/\"train/neg/\")\n",
    "pos_y = np.ones(len(pos_rev))\n",
    "neg_y = np.zeros(len(neg_rev))\n",
    "X = np.append(pos_rev, neg_rev)\n",
    "y = np.append(pos_y, neg_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500,), (12500,), (25000,), (25000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_y.shape, neg_y.shape, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([\"I have to say, Seventeen & Missing is much better than I expected. The perception I took from the previews was that it would be just humdrum but I was pleasantly surprised with this impressive mystery.<br /><br />Dedee Pfeiffer is Emilie, a mom who insists her daughter, Lori (Tegan Moss), not attend a so-called graduation party one weeknight, but Lori ignores her mother's wishes and takes off for the party anyway. When Lori does not come home, Emilie knows something is wrong and she begins to have visions of her daughter and the events that led to her disappearance.<br /><br />Seventeen & Missing is better than so many other TV movies of this type, as it is not so predictable. Pfeiffer is the reason to see this movie, and most of it comes off as believable. This LMN Original Movie premiered last night. 10/10\",\n",
       "        \"I sometimes grow weary of reading reviews of some of Hitchcock's lesser known films, because almost every single one starts out with someone saying this film is grossly overlooked or this is a hidden Hitchcock gem or a true Hitchcock great or some other generic if - only - people - would - watch - this - they - would - see - that - this - is - a - great - Hitchcock - film - just - as - much - as - Vertigo - North - by - Northwest - Psycho - Rear - Window - etc. So, that being said, I would just like to say that if - only - people - would - watch - this - they - would - see - that - this - is - a - great - Hitchcock - film - just - as - much - as - Vertigo - North - by - Northwest - Psycho - Rear - Window - etc.<br /><br />Now, that may be overshooting a little bit, The Ring is not by any stretch of the imagination even in the same league as any of those films mentioned twice above, but compared to the other films that Hitchcock made in the late 1920s and early 1930s, I really think that The Ring is one of the best photographed and performed films of mostly all of them. As an almost brand new director, there are some astonishing dream sequences and brilliant segments of editing which show why Hitchcock was generating so much attention early in his career.<br /><br />Granted, the film does start with, among other things, the highly disturbing spectacle of an idiot black circus performer (and I use idiot in the definitive manner, the way Stephen King so often does) having eggs and fruit thrown at him by a crowd of not the classiest looking white people. I suppose this only illustrates how incredibly different such circuses and people were back then, but I think it is one of the most off-putting sequences in any Hitchcock film I've seen.<br /><br />The main attraction at the circus is a fighter who claims to be able to knock any man down in one round, but when he meets his match, it is against a man that challenges his authority not only in the boxing ring but also in the ring around his wife's finger. So begins an entertaining if not very tense challenge for the love of one woman, who seems to sway from one man to the other effortlessly and thoughtlessly.<br /><br />(spoilers) There is, for example, a scene where her husband watches her from above as she is dropped off at home late at night and, just before going into the building, she is coaxed back to the car for a kiss. This kiss is never explained, and there is also the fact that, even at the end when she proves faithful to her husband, or at least ultimately chooses him, they look into each other's eyes but do not actually kiss.<br /><br />The film is certainly beautifully photographed, even more so than several films that Hitch released in subsequent years. There is also a performance by Gordon Harker as One Round Jack's trainer who, in his stone faced expressionism, reminds me quite often of the brilliant Buster Keaton. Hitch leaves it a bit ambiguous, but this is a great sample of his early work.\"],\n",
       "       dtype='<U13704'), array([1., 1.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2], y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vocab from training sets\n",
    "data_vocab = get_vocab([X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing embedding layer with Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\r\n"
     ]
    }
   ],
   "source": [
    "! head -1 /home/ubuntu/hw2-sentiment-classification-groverpr/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to initialize the embeddings from our model with the pre-trained Glove embeddings. After initializing we should \"freeze\" the embeddings at least initially. The rationale is that we first want the network to learn weights for the other parameters that were randomly initialize. After that phase we could finetune the embeddings to our task.\n",
    "\n",
    "`embed.weight.requires_grad = False` freezes the embedding parameters.\n",
    "\n",
    "The following code initializes the embedding. Here V is the vocabulary size and D is the embedding size. pretrained_weight is a numpy matrix of shape (V, D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile=\"/home/ubuntu/hw2-sentiment-classification-groverpr/glove.6B.100d.txt\"):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 73990\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vecs.keys()), len(data_vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rare_words(word_vecs, data_vocab, min_df=2):\n",
    "    \"\"\" Deletes rare words from data_vocab\n",
    "    \n",
    "    Deletes words from data_vocab if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies in data_vocab.\n",
    "    \"\"\"\n",
    "    words_delete = []\n",
    "    for word in data_vocab:\n",
    "        if data_vocab[word] < min_df and word not in word_vecs:\n",
    "            words_delete.append(word)\n",
    "#     print(words_delete)\n",
    "    for word in words_delete: data_vocab.pop(word)\n",
    "    return data_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73990"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clean up issues here\n",
    "data_vocab = delete_rare_words(word_vecs, data_vocab, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59526"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_vecs, data_vocab, min_df=2, D=100):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    data_vocab = delete_rare_words(word_vecs, data_vocab, min_df)\n",
    "    V = len(data_vocab.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, D), dtype=\"float32\")\n",
    "    vocab = [\"\", \"unk\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(D, dtype='float32') # first vector\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25,0.25,D) # second vector\n",
    "    vocab2index[\"unk\"] = 1\n",
    "    i = 2\n",
    "    for word in data_vocab:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else: # words for which we couldn't find an embeddng\n",
    "            W[i] = np.random.uniform(-0.25,0.25,D)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, data_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59528"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_weight) # note that index 0 is for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.1347  0.2155 -0.0796  ...  -0.1745  0.2082 -0.0251\n",
       "-0.0394 -0.4136 -0.3717  ...   0.1673 -0.0799  0.3219\n",
       "          ...             ⋱             ...          \n",
       " 0.7739 -0.6130 -0.4644  ...   0.5705 -0.8239  0.3046\n",
       " 0.0805 -0.4075 -0.3767  ...  -0.0297 -0.6540 -0.2768\n",
       "-0.1604  0.0403 -0.7888  ...   0.0545 -0.9363 -0.9908\n",
       "[torch.FloatTensor of size 59528x100]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = 100\n",
    "V = len(pretrained_weight)\n",
    "emb = nn.Embedding(V, D)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 1D Convolutional neural networks as our model. CNNs assume a fixed input size so we need to assume a fixed size and truncate or pad the sentences as needed. Let's find a good value to set our sequence length to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.array([len(x.split()) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(x_len, 95) # let set the max sequence len to N=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Being a big fan of the \"other\" PLANET OF THE APES films, I rented this DVD despite my aversion to all things Tim Burton. Once again, he doesn\\'t fail to disappoint with his uninspired direction. Even the ape makeup looks second rate, which is unforgivable considering the monstrous budget of this monstrosity. Mark Wahlberg proves once and for all that he is not an actor (as if BOOGIE NIGHTS wasn\\'t proof enough). I was embarrassed for genuine talents such as Tim Roth and Helena Bonham Carter. No doubt their paychecks motivated them since it couldn\\'t possibly have been the cliche-ridden screenplay. I rented this DVD on a special $1 night and I still feel ripped off.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the index of the word or the index of \"UNK\" otherwise\n",
    "vocab2index.get(\"will\", vocab2index[\"unk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    94,    77,    26,    15,    84,     1,     1,     1,\n",
       "           1,     1,     1,     1,    54,    22,     1,    50,    48,\n",
       "          53,    86,    24,    31,     1,     1,     1,     1,    23,\n",
       "       27037,    78,    86,    19,    79,    34,    55,     1,     1,\n",
       "          84,    75,    52,    49,    97,     1,    21,    85,    69,\n",
       "          44,    84,    30,     5,    15,    22,     1,     1,     1,\n",
       "          25,     7,    42,    46,    24,    71,    23,    85,    29,\n",
       "          80,    17,     1,    13,     1,     1, 16942,    83,     1,\n",
       "           1,    88,    39,    46,    65,     8,    95,    37,     1,\n",
       "           1,    42,     1,     1,     1,     1,     3,    32,    12,\n",
       "          27,    74,     6,    56, 43297,    47,    11,    38,    84,\n",
       "           1,     1,     1,    54,    22,     1,    28,    94,    82,\n",
       "           1,    91,    42,     1,     4,    93,    63,     1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([vocab2index.get(w, vocab2index[\"unk\"]) for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, N=590):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"unk\"]) for w in s.split()])\n",
    "    t = min(N, len(enc1))\n",
    "    enc[:t] = enc1[:t]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    94,    77,    26,    15,    84,     1,     1,     1,\n",
       "           1,     1,     1,     1,    54,    22,     1,    50,    48,\n",
       "          53,    86,    24,    31,     1,     1,     1,     1,    23,\n",
       "       27037,    78,    86,    19,    79,    34,    55,     1,     1,\n",
       "          84,    75,    52,    49,    97,     1,    21,    85,    69,\n",
       "          44,    84,    30,     5,    15,    22,     1,     1,     1,\n",
       "          25,     7,    42,    46,    24,    71,    23,    85,    29,\n",
       "          80,    17,     1,    13,     1,     1, 16942,    83,     1,\n",
       "           1,    88,    39,    46,    65,     8,    95,    37,     1,\n",
       "           1,    42,     1,     1,     1,     1,     3,    32,    12,\n",
       "          27,    74,     6,    56, 43297,    47,    11,    38,    84,\n",
       "           1,     1,     1,    54,    22,     1,    28,    94,    82,\n",
       "           1,    91,    42,     1,     4,    93,    63,     1,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 590)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 590)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.vstack([encode_sentence(x) for x in X_test])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing and debugging CNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = len(pretrained_weight)\n",
    "D = 100\n",
    "N = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59528, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.1347  0.2155 -0.0796  ...  -0.1745  0.2082 -0.0251\n",
       "-0.0394 -0.4136 -0.3717  ...   0.1673 -0.0799  0.3219\n",
       "          ...             ⋱             ...          \n",
       " 0.7739 -0.6130 -0.4644  ...   0.5705 -0.8239  0.3046\n",
       " 0.0805 -0.4075 -0.3767  ...  -0.0297 -0.6540 -0.2768\n",
       "-0.1604  0.0403 -0.7888  ...   0.0545 -0.9363 -0.9908\n",
       "[torch.FloatTensor of size 59528x100]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(W, D)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 590)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_train[:2]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     1     94     77  ...       0      0      0\n",
       "     1    146     86  ...       0      0      0\n",
       "[torch.LongTensor of size 2x590]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 590, 100])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = emb(V(x))\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 590])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x1.transpose(1,2)  # needs to convert x to (batch, embedding_dim, sentence_len)\n",
    "x1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = conv_3(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 588])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_4 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=4)\n",
    "conv_5 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 587]) torch.Size([2, 50, 586])\n"
     ]
    }
   ],
   "source": [
    "x4 = conv_4(x1)\n",
    "x5 = conv_5(x1)\n",
    "print(x4.size(), x5.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution all apply to the same x1. How do we combine now the results of the convolutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 3-gram detectors # 100 3-g \n",
    "x3 = nn.ReLU()(x3)\n",
    "x3 = nn.MaxPool1d(kernel_size = 588)(x3)\n",
    "x3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 4-gram detectors\n",
    "x4 = nn.ReLU()(x4)\n",
    "x4 = nn.MaxPool1d(kernel_size = 587)(x4)\n",
    "x4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 5-gram detectors\n",
    "x5 = nn.ReLU()(x5)\n",
    "x5 = nn.MaxPool1d(kernel_size = 586)(x5)\n",
    "x5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate x3, x4, x5\n",
    "out = torch.cat([x3, x4, x5], 2)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 150])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(out.size(0), -1)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we have a fully connected network. Let's write a network that implements this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, x, y):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        review = self.x[index]\n",
    "        sentiment = self.y[index]\n",
    "        return review, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 590)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = torch.LongTensor(x_train)\n",
    "train_sentiment = torch.FloatTensor(y_train)\n",
    "\n",
    "train_dataset = Dataset(train_reviews, train_sentiment)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     1     84   3484  ...       0      0      0\n",
       "     1  14856     20  ...       0      0      0\n",
       "     1      1  23693  ...       0      0      0\n",
       "        ...            ⋱           ...         \n",
       "     1    370   1094  ...       0      0      0\n",
       "     1   1787     85  ...       0      0      0\n",
       "     1    177   1495  ...       0      0      0\n",
       "[torch.LongTensor of size 64x590]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       "[torch.DoubleTensor of size 4]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = torch.LongTensor(x_test)\n",
    "test_sentiment = torch.FloatTensor(y_test)\n",
    "\n",
    "test_dataset = Dataset(test_reviews, test_sentiment)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D CNN model for sentence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation:\n",
    "\n",
    "W -- vocabulary size\n",
    "D -- embedding size\n",
    "N -- MAX Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, W, D, glove_weights):\n",
    "        super(SentenceCNN, self).__init__()\n",
    "        self.glove_weights = glove_weights\n",
    "        self.embedding = nn.Embedding(W, D, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(self.glove_weights))\n",
    "        self.embedding.weight.requires_grad = False ## freeze embeddings\n",
    "\n",
    "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=3)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=4)\n",
    "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(150, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x3 = F.relu(self.conv_3(x))\n",
    "        x4 = F.relu(self.conv_4(x))\n",
    "        x5 = F.relu(self.conv_5(x))\n",
    "        x3 = nn.MaxPool1d(kernel_size = 588)(x3)\n",
    "        x4 = nn.MaxPool1d(kernel_size = 587)(x4)\n",
    "        x5 = nn.MaxPool1d(kernel_size = 586)(x5)\n",
    "        out = torch.cat([x3, x4, x5], 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = len(pretrained_weight)\n",
    "D = 100\n",
    "N = 590\n",
    "model = SentenceCNN(W, D, glove_weights=pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 590)\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "x = x_train[:10]\n",
    "print(x.shape)\n",
    "x = torch.LongTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(x)\n",
    "y_hat.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceCNN(W, D, glove_weights=pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        ctr = 0.\n",
    "        model.train() # into training mode\n",
    "        running_loss = 0.0        \n",
    "        for j, data in enumerate(train_loader):\n",
    "            ctr+=1\n",
    "            x,y = data\n",
    "            x = V(x).cuda()\n",
    "            y = V(y).float().cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y.unsqueeze(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data[0]\n",
    "        print(\"training loss for epoch \",i+1, \": \", running_loss/ctr) # used to be loss.data[0]\n",
    "        test_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining test loss which has been used in train_epochs\n",
    "\n",
    "def test_loss(model):\n",
    "    model.eval() # go to evaluation mode\n",
    "    \n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for j, data in enumerate(test_loader):\n",
    "        x,y = data\n",
    "        x,y = V(x), V(y).float()        \n",
    "        x = x.cuda() # put on gpu\n",
    "        y = y.cuda()\n",
    "        y_hat = model(x)\n",
    "        loss_now = F.binary_cross_entropy_with_logits(y_hat, y.unsqueeze(-1))\n",
    "        y_pred = y_hat > 0\n",
    "        correct_now = (y_pred.float() == y.unsqueeze(-1)).sum()\n",
    "        correct += correct_now.data[0]\n",
    "        total += len(y.float())\n",
    "        running_loss+= loss_now.data[0]\n",
    "        \n",
    "    accuracy = correct/total\n",
    "    print(\"validation loss\", \": \", running_loss/len(test_loader), 'Accuracy',  accuracy, '\\n') # j means this many iterations till end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.503"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test == 1)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(model, x_train, y_train):\n",
    "model = SentenceCNN(W, D, glove_weights=pretrained_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss :  0.6961383547963975 Accuracy 0.5026 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial model accuracy without training at all\n",
    "test_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfinder(start, end, model, train_loader, epochs=2):\n",
    "    model.train() # into training mode\n",
    "    lrs = np.linspace(start, end, epochs*len(train_loader))    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, start)\n",
    "    losses = []\n",
    "    ctr = 0\n",
    "    for i in range(epochs): \n",
    "        for j, data in enumerate(train_loader):\n",
    "            optimizer.param_groups[0]['lr'] =lrs[ctr]\n",
    "            ctr = ctr+1            \n",
    "            x,y = data\n",
    "            x = V(x).cuda()\n",
    "            y = V(y).float().cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y.unsqueeze(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data[0])\n",
    "    plt.plot(lrs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copy = SentenceCNN(W, D, glove_weights=pretrained_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrfinder(0.0001, 0.001, model_copy, train_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  0.6507074804351733\n",
      "validation loss :  0.5611682639846319 Accuracy 0.7802 \n",
      "\n",
      "training loss for epoch  2 :  0.5202482648360462\n",
      "validation loss :  0.4602180078059812 Accuracy 0.8006 \n",
      "\n",
      "training loss for epoch  3 :  0.4569666712238385\n",
      "validation loss :  0.4199211638939532 Accuracy 0.8172 \n",
      "\n",
      "training loss for epoch  4 :  0.4248657495069047\n",
      "validation loss :  0.4014192589476139 Accuracy 0.8226 \n",
      "\n",
      "training loss for epoch  5 :  0.39974487722872165\n",
      "validation loss :  0.3884179463869409 Accuracy 0.8244 \n",
      "\n",
      "training loss for epoch  6 :  0.3806269116961537\n",
      "validation loss :  0.37913551323021516 Accuracy 0.8314 \n",
      "\n",
      "training loss for epoch  7 :  0.3672755530086188\n",
      "validation loss :  0.36722659423381465 Accuracy 0.8376 \n",
      "\n",
      "training loss for epoch  8 :  0.35427288106455207\n",
      "validation loss :  0.36494443612762645 Accuracy 0.838 \n",
      "\n",
      "training loss for epoch  9 :  0.3429942146276894\n",
      "validation loss :  0.3583196910876262 Accuracy 0.8416 \n",
      "\n",
      "training loss for epoch  10 :  0.33345241301928086\n",
      "validation loss :  0.35139367191851895 Accuracy 0.8444 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  0.3217825372093402\n",
      "validation loss :  0.3509282658371744 Accuracy 0.8472 \n",
      "\n",
      "training loss for epoch  2 :  0.3136856664959996\n",
      "validation loss :  0.35660397742368 Accuracy 0.8482 \n",
      "\n",
      "training loss for epoch  3 :  0.30421863382045455\n",
      "validation loss :  0.3460835789955115 Accuracy 0.8486 \n",
      "\n",
      "training loss for epoch  4 :  0.2963184809532409\n",
      "validation loss :  0.3500470864244654 Accuracy 0.8446 \n",
      "\n",
      "training loss for epoch  5 :  0.28928900740969293\n",
      "validation loss :  0.3445833021704155 Accuracy 0.8498 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  0.28076633720542676\n",
      "validation loss :  0.3449968901238864 Accuracy 0.8508 \n",
      "\n",
      "training loss for epoch  2 :  0.2701239127891894\n",
      "validation loss :  0.34178599187090425 Accuracy 0.8506 \n",
      "\n",
      "training loss for epoch  3 :  0.2642558252754303\n",
      "validation loss :  0.34063526591922666 Accuracy 0.8506 \n",
      "\n",
      "training loss for epoch  4 :  0.26073888517892396\n",
      "validation loss :  0.33916224558142166 Accuracy 0.8514 \n",
      "\n",
      "training loss for epoch  5 :  0.24906223969528088\n",
      "validation loss :  0.34052729399143894 Accuracy 0.8508 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreezing embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreezing the embeddings\n",
    "model.embedding.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  0.2469370202801098\n",
      "validation loss :  0.33459822607191303 Accuracy 0.8562 \n",
      "\n",
      "training loss for epoch  2 :  0.22455412968279073\n",
      "validation loss :  0.3356941104689731 Accuracy 0.8566 \n",
      "\n",
      "training loss for epoch  3 :  0.20543431673948756\n",
      "validation loss :  0.3309607817025124 Accuracy 0.8594 \n",
      "\n",
      "training loss for epoch  4 :  0.19113999847977306\n",
      "validation loss :  0.32534639669370047 Accuracy 0.8612 \n",
      "\n",
      "training loss for epoch  5 :  0.17280511829418876\n",
      "validation loss :  0.3343993966715245 Accuracy 0.8578 \n",
      "\n",
      "training loss for epoch  6 :  0.1581073027972977\n",
      "validation loss :  0.32435554416873785 Accuracy 0.8646 \n",
      "\n",
      "training loss for epoch  7 :  0.14780488498389913\n",
      "validation loss :  0.33809788294042215 Accuracy 0.857 \n",
      "\n",
      "training loss for epoch  8 :  0.136364911608517\n",
      "validation loss :  0.33202205145660835 Accuracy 0.8664 \n",
      "\n",
      "training loss for epoch  9 :  0.12407234805032087\n",
      "validation loss :  0.3400866117658494 Accuracy 0.8628 \n",
      "\n",
      "training loss for epoch  10 :  0.11535173765243814\n",
      "validation loss :  0.3377403357549559 Accuracy 0.8646 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.0003, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing learning rate a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  0.10132902566664898\n",
      "validation loss :  0.3402680598105056 Accuracy 0.864 \n",
      "\n",
      "training loss for epoch  2 :  0.10060794990712081\n",
      "validation loss :  0.3385278201555904 Accuracy 0.8652 \n",
      "\n",
      "training loss for epoch  3 :  0.10403536740964213\n",
      "validation loss :  0.33704910606523103 Accuracy 0.8662 \n",
      "\n",
      "training loss for epoch  4 :  0.09940833791185873\n",
      "validation loss :  0.3367641541776778 Accuracy 0.8654 \n",
      "\n",
      "training loss for epoch  5 :  0.09983570232225683\n",
      "validation loss :  0.33482024186774145 Accuracy 0.8656 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.0001, wd=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
